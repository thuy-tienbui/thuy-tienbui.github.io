{
  "hash": "f8b89c8a89e6e53ae4d2f90b0e3a329f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting forest burn area using random forests\"\ndescription: |\n  A machine-learning analysis for predicting the amount of area burned by wildfire\nauthor:\n  - name: Thuy-Tien Bui\ndate: 2025-03-01\ncategories: [R, Modeling, Fire, Climate]\nimage: portugal_fire.jpg\nformat:\n  html:\n    embed-resources: true\n    code-fold: true\n    toc: false\n    page-layout: full\necho: true\nmessage: false\nwarning: false\n---\n\n\n\n### Introduction to the dataset\n\nThis analysis utilizes meteorological and fire-related data collected from forest fire incidents in the northeast region of Portugal. Key environmental variables include temperature, wind speed, relative humidity, and precipitation, which are known to influence fire behavior. Additionally, the dataset incorporates fire danger ratings from the Canadian Forest Fire Weather Index (FWI) system, which includes components such as the Fine Fuel Moisture Code (FFMC), Duff Moisture Code (DMC), Drought Code (DC), Initial Spread Index (ISI), Buildup Index (BUI), and the overall FWI score. These indices account for fuel moisture, fire spread potential, and fire intensity, incorporating past weather conditions to estimate fire behavior. By leveraging these meteorological and fire danger indicators, the dataset provides a robust foundation for modeling and predicting burn area using machine learning techniques.\n\n**Data Citation:** Cortez, P., & Raimundo Morais, A. de J. (2007). A data mining approach to predict forest fires using meteorological data. *Associação Portuguesa Para a Inteligência Artificial (APPIA)*.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) \nlibrary(janitor)\nlibrary(dplyr)\nlibrary(tidymodels)\nlibrary(ggcorrplot)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ranger)\nlibrary(here)\nlibrary(ggplot2)\nlibrary(yardstick)\nlibrary(vip)\n```\n:::\n\n\n\n### Purpose\n\nThe objective of this analysis is to develop and tune a regression Random Forest model to predict the extent of forest area burned based on STFWI variables (spatial, temporal and the four FWI components). By leveraging meteorological and environmental variables, this study aims to identify key factors influencing fire spread and improve forecasting accuracy. Accurate predictions of burn area can assist in resource allocation for firefighting efforts, prioritizing response strategies, and mitigating the environmental and economic impacts of forest fires.\n\n### Data exploration\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the data and convert the spatial and temporal variables to factors. Log(x+1) transform the burned hectares data\nforest <- read_csv(here(\"data\", \"forestfires.csv\")) %>% \n  clean_names() %>% \n  mutate(month = as.factor(month),\n         day = as.factor(day)) %>%\n  mutate(area = log(area + 1))\n\n#make fig corr map using ggcorrplot\nforest %>% select(where(is.numeric)) %>% \n  cor() %>% \n  ggcorrplot(\n    method = \"circle\",\n    type='upper',\n    outline.col = \"black\",\n    legend.title = \"Correlation\" \n  ) +\n   labs(\n    title = \"Variables Correlation Matrix\")\n```\n\n::: {.cell-output-display}\n![Figure 1. Correlation matrix heatmap. Red corresponds to a positive correlation while blue represents a negative correlation between the two variables. Darker colors indicate stronger correlations. Temperature has a strong negative correlation with relative humidity (rh) while duff moisture code (dmc) and drought code (dc) have a strong positive correlation.](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n### Methods\n\n1.  Split the data into training and testing sets.\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Split the data into training and testing sets\n    set.seed(123)\n    forest_split <- initial_split(forest, prop = 0.8, strata = area)\n    forest_train <- training(forest_split)\n    forest_test <- testing(forest_split)\n    ```\n    :::\n\n\n\n2.  Create a preprocessing recipe to handle zero-variance predictors and high correlations.\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Create a recipe for preprocessing the data\n    forest_recipe <- recipe(area ~ ., data = forest_train) %>%  \n      step_zv(all_predictors()) %>%  \n      step_corr(all_numeric_predictors(), threshold = 0.9)\n    ```\n    :::\n\n\n\n3.  Define the Random Forest model and set tuning parameters.\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Set engine\n    forest_spec <- rand_forest(mtry = tune(), trees = 500, min_n = tune()) %>% \n      set_engine(\"ranger\") %>% \n      set_mode(\"regression\")\n    ```\n    :::\n\n\n\n4.  Create a workflow by combining the preprocessing recipe and model.\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Create a workflow\n    forest_wf <- workflow() %>% \n      add_recipe(forest_recipe) %>% \n      add_model(forest_spec)\n    ```\n    :::\n\n\n\n5.  Define a hyperparameter grid for tuning.\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Create a grid of hyperparameters to tune\n    forest_grid= expand_grid(\n      mtry = seq(1, 6, by=2),\n      min_n = seq(2, 8, by=2)\n    )\n    ```\n    :::\n\n\n\n6.  Perform cross-validation and tune the model using the defined grid.\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Perform cross-validation\n    forest_res <- tune_grid(\n      forest_wf,\n      resamples = vfold_cv(forest_train, v = 10),\n      grid = forest_grid,\n      metrics = metric_set(mae, rmse, rsq),\n      control = control_grid(save_workflow = TRUE))\n    ```\n    :::\n\n\n\n7.  Select the best hyperparameters based on mean absolute error (MAE) and finalize the model.\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Finalize model using mean absolute error metric\n    forest_best <- select_best(forest_res, metric='mae')\n    forest_final <- finalize_model(forest_spec, forest_best)\n    ```\n    :::\n\n\n\n8.  Fit the final model to the training data and evaluate it on the test set.\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Fit the final model on the training data and evaluate on test set\n    final_wf <- workflow() %>%\n      add_recipe(forest_recipe) %>%\n      add_model(forest_final)\n    \n    final_res <- final_wf %>%\n      last_fit(forest_split, metrics = metric_set(mae))\n    ```\n    :::\n\n\n\n9.  Collect the final predictions from the test set.\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Collect the results\n    final_predictions <- final_res %>%\n      collect_predictions()\n    \n    best_mtry <- forest_best$mtry\n    best_min_n <- forest_best$min_n\n    \n    metrics <- final_res %>%\n      collect_metrics()\n    \n    mae_value <- mae(final_predictions, truth = area, estimate = .pred)\n    rmse_value <- rmse(final_predictions, truth = area, estimate = .pred)\n    \n    # Transform to compare to paper\n    final_predictions_nonlog <- final_predictions %>%\n      mutate(\n        .pred = (exp(.pred)-1),\n        area = (exp(area)-1)\n      )\n    \n    mae_nonlog <- mae(final_predictions_nonlog, truth = area, estimate = .pred)\n    rmse_nonlog <- rmse(final_predictions_nonlog, truth = area, estimate = .pred)\n    \n    results_table <- tibble(\n      `MAE (Test)` = mae_value$.estimate,\n      `MAE (Non-Log)` = mae_nonlog$.estimate,\n      `RMSE (Test)` = rmse_value$.estimate,\n      `RMSE (Non-Log)` = rmse_nonlog$estimate,\n      `Optimal mtry` = best_mtry,\n      `Optimal min_n` = best_min_n,\n      `Study MAE` = 13.31 \n    )\n    \n    kable(results_table, caption = \"Random Forest Model Performance and Hyperparameters\") %>% \n      kable_styling()\n    ```\n    \n    ::: {.cell-output-display}\n    `````{=html}\n    <table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n    <caption>Random Forest Model Performance and Hyperparameters</caption>\n     <thead>\n      <tr>\n       <th style=\"text-align:right;\"> MAE (Test) </th>\n       <th style=\"text-align:right;\"> MAE (Non-Log) </th>\n       <th style=\"text-align:right;\"> RMSE (Test) </th>\n       <th style=\"text-align:right;\"> Optimal mtry </th>\n       <th style=\"text-align:right;\"> Optimal min_n </th>\n       <th style=\"text-align:right;\"> Study MAE </th>\n      </tr>\n     </thead>\n    <tbody>\n      <tr>\n       <td style=\"text-align:right;\"> 1.075592 </td>\n       <td style=\"text-align:right;\"> 10.38123 </td>\n       <td style=\"text-align:right;\"> 1.336439 </td>\n       <td style=\"text-align:right;\"> 1 </td>\n       <td style=\"text-align:right;\"> 2 </td>\n       <td style=\"text-align:right;\"> 13.31 </td>\n      </tr>\n    </tbody>\n    </table>\n    \n    `````\n    :::\n    :::\n\n\n\n> By tuning mtry and min_n to minimize MAE, the model's predictive accuracy improved, as seen in the reduction from 13.31 (Study MAE) to 10.38 (MAE Non-Log). This tuning helped to identify the optimal number of predictors to consider at each split and ensured that each decision tree captured meaningful patterns that avoided overfitting. The improvement indicates that the tuned hyperparameters led to better predictions by the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot variable importance\nforest_final %>% \n  set_engine('ranger', importance = 'permutation') %>% \n  fit(area ~ ., data = juice(prep(forest_recipe))) %>%\n  vip(geom = \"point\", aesthetics = list(color = \"darkblue\", \n                                        size = 3, \n                                        alpha=0.5)) +\n  labs(title = \"Variable Importance Plot\",\n       x = \"Variable\",\n       y = \"Importance Score\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 16),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n  )\n```\n\n::: {.cell-output-display}\n![Figure 2. Variable Importance Plot for Random Forest Model Predicting Burn Area. The plot displays the relative importance of each predictor variable in the model, with the importance scores calculated using permutation-based feature importance. Variables are ordered from least important to most important on the y-axis with their importance score on the x-axis highlighting which variables are most influential in predicting burn area.](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n### Conclusions\n\n-   Tuning the model on hyperparameters (e.g., mtry, min_n) resulted in a decrease in MAE from 13.31 to 10.43, indicating an improvement in predictive accuracy.\n\n-   The variable importance plot highlights which predictors contribute most to the model's predictive power. These variables are drought code, temperature, and month. Ecologically, these variables may have a bigger influence on fire and burn area compared to the other variables included in this analysis. Identifying and prioritizing key variables can reduce the number of features in the model without sacrificing accuracy, leading to a more efficient model.\n\n-   Rain and relative humidity have the lowest variable importance scores, indicating that these factors contribute less to the model's predictive power. While these variables may play a role in influencing burn area, their impact is less significant compared to other predictors in the model. These variables can be removed in future model iterations to reduce complexity and minimize overfitting.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}