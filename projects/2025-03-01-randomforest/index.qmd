---
title: "Predicting forest burn area using random forests"
description: |
  A machine-learning analysis for predicting the amount of area burned by wildfire
author:
  - name: Thuy-Tien Bui
date: 2025-03-01
categories: [R, Modeling, Fire, Climate]
image: portugal_fire.jpg
format:
  html:
    embed-resources: true
    code-fold: true
    toc: false
    page-layout: full
echo: true
message: false
warning: false
---

### Introduction to the dataset

This analysis utilizes meteorological and fire-related data collected from forest fire incidents in the northeast region of Portugal. Key environmental variables include temperature, wind speed, relative humidity, and precipitation, which are known to influence fire behavior. Additionally, the dataset incorporates fire danger ratings from the Canadian Forest Fire Weather Index (FWI) system, which includes components such as the Fine Fuel Moisture Code (FFMC), Duff Moisture Code (DMC), Drought Code (DC), Initial Spread Index (ISI), Buildup Index (BUI), and the overall FWI score. These indices account for fuel moisture, fire spread potential, and fire intensity, incorporating past weather conditions to estimate fire behavior. By leveraging these meteorological and fire danger indicators, the dataset provides a robust foundation for modeling and predicting burn area using machine learning techniques.

**Data Citation:** Cortez, P., & Raimundo Morais, A. de J. (2007). A data mining approach to predict forest fires using meteorological data. *Associação Portuguesa Para a Inteligência Artificial (APPIA)*.

```{r}
library(tidyverse) 
library(janitor)
library(dplyr)
library(tidymodels)
library(ggcorrplot)
library(knitr)
library(kableExtra)
library(ranger)
library(here)
library(ggplot2)
library(yardstick)
library(vip)
```

### Purpose

The objective of this analysis is to develop and tune a regression Random Forest model to predict the extent of forest area burned based on STFWI variables (spatial, temporal and the four FWI components). By leveraging meteorological and environmental variables, this study aims to identify key factors influencing fire spread and improve forecasting accuracy. Accurate predictions of burn area can assist in resource allocation for firefighting efforts, prioritizing response strategies, and mitigating the environmental and economic impacts of forest fires.

### Data exploration

```{r}
#| fig-cap: "Figure 1. Correlation matrix heatmap. Red corresponds to a positive correlation while blue represents a negative correlation between the two variables. Darker colors indicate stronger correlations. Temperature has a strong negative correlation with relative humidity (rh) while duff moisture code (dmc) and drought code (dc) have a strong positive correlation."

# Load the data and convert the spatial and temporal variables to factors. Log(x+1) transform the burned hectares data
forest <- read_csv(here("data", "forestfires.csv")) %>% 
  clean_names() %>% 
  mutate(month = as.factor(month),
         day = as.factor(day)) %>%
  mutate(area = log(area + 1))

#make fig corr map using ggcorrplot
forest %>% select(where(is.numeric)) %>% 
  cor() %>% 
  ggcorrplot(
    method = "circle",
    type='upper',
    outline.col = "black",
    legend.title = "Correlation" 
  ) +
   labs(
    title = "Variables Correlation Matrix")
```

### Methods

1.  Split the data into training and testing sets.

    ```{r}
    # Split the data into training and testing sets
    set.seed(123)
    forest_split <- initial_split(forest, prop = 0.8, strata = area)
    forest_train <- training(forest_split)
    forest_test <- testing(forest_split)
    ```

2.  Create a preprocessing recipe to handle zero-variance predictors and high correlations.

    ```{r}
    # Create a recipe for preprocessing the data
    forest_recipe <- recipe(area ~ ., data = forest_train) %>%  
      step_zv(all_predictors()) %>%  
      step_corr(all_numeric_predictors(), threshold = 0.9)
    ```

3.  Define the Random Forest model and set tuning parameters.

    ```{r}
    # Set engine
    forest_spec <- rand_forest(mtry = tune(), trees = 500, min_n = tune()) %>% 
      set_engine("ranger") %>% 
      set_mode("regression")
    ```

4.  Create a workflow by combining the preprocessing recipe and model.

    ```{r}
    # Create a workflow
    forest_wf <- workflow() %>% 
      add_recipe(forest_recipe) %>% 
      add_model(forest_spec)
    ```

5.  Define a hyperparameter grid for tuning.

    ```{r}
    # Create a grid of hyperparameters to tune
    forest_grid= expand_grid(
      mtry = seq(1, 6, by=2),
      min_n = seq(2, 8, by=2)
    )
    ```

6.  Perform cross-validation and tune the model using the defined grid.

    ```{r}
    # Perform cross-validation
    forest_res <- tune_grid(
      forest_wf,
      resamples = vfold_cv(forest_train, v = 10),
      grid = forest_grid,
      metrics = metric_set(mae, rmse, rsq),
      control = control_grid(save_workflow = TRUE))
    ```

7.  Select the best hyperparameters based on mean absolute error (MAE) and finalize the model.

    ```{r}
    # Finalize model using mean absolute error metric
    forest_best <- select_best(forest_res, metric='mae')
    forest_final <- finalize_model(forest_spec, forest_best)
    ```

8.  Fit the final model to the training data and evaluate it on the test set.

    ```{r}
    # Fit the final model on the training data and evaluate on test set
    final_wf <- workflow() %>%
      add_recipe(forest_recipe) %>%
      add_model(forest_final)

    final_res <- final_wf %>%
      last_fit(forest_split, metrics = metric_set(mae))
    ```

9.  Collect the final predictions from the test set.

    ```{r}
    # Collect the results
    final_predictions <- final_res %>%
      collect_predictions()

    best_mtry <- forest_best$mtry
    best_min_n <- forest_best$min_n

    metrics <- final_res %>%
      collect_metrics()

    mae_value <- mae(final_predictions, truth = area, estimate = .pred)
    rmse_value <- rmse(final_predictions, truth = area, estimate = .pred)

    # Transform to compare to paper
    final_predictions_nonlog <- final_predictions %>%
      mutate(
        .pred = (exp(.pred)-1),
        area = (exp(area)-1)
      )

    mae_nonlog <- mae(final_predictions_nonlog, truth = area, estimate = .pred)
    rmse_nonlog <- rmse(final_predictions_nonlog, truth = area, estimate = .pred)

    results_table <- tibble(
      `MAE (Test)` = mae_value$.estimate,
      `MAE (Non-Log)` = mae_nonlog$.estimate,
      `RMSE (Test)` = rmse_value$.estimate,
      `RMSE (Non-Log)` = rmse_nonlog$estimate,
      `Optimal mtry` = best_mtry,
      `Optimal min_n` = best_min_n,
      `Study MAE` = 13.31 
    )

    kable(results_table, caption = "Random Forest Model Performance and Hyperparameters") %>% 
      kable_styling()
    ```

> By tuning mtry and min_n to minimize MAE, the model's predictive accuracy improved, as seen in the reduction from 13.31 (Study MAE) to 10.38 (MAE Non-Log). This tuning helped to identify the optimal number of predictors to consider at each split and ensured that each decision tree captured meaningful patterns that avoided overfitting. The improvement indicates that the tuned hyperparameters led to better predictions by the model.

```{r}
#| fig-cap: "Figure 2. Variable Importance Plot for Random Forest Model Predicting Burn Area. The plot displays the relative importance of each predictor variable in the model, with the importance scores calculated using permutation-based feature importance. Variables are ordered from least important to most important on the y-axis with their importance score on the x-axis highlighting which variables are most influential in predicting burn area."

# Plot variable importance
forest_final %>% 
  set_engine('ranger', importance = 'permutation') %>% 
  fit(area ~ ., data = juice(prep(forest_recipe))) %>%
  vip(geom = "point", aesthetics = list(color = "darkblue", 
                                        size = 3, 
                                        alpha=0.5)) +
  labs(title = "Variable Importance Plot",
       x = "Variable",
       y = "Importance Score") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 16),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
  )

```

### Conclusions

-   Tuning the model on hyperparameters (e.g., mtry, min_n) resulted in a decrease in MAE from 13.31 to 10.43, indicating an improvement in predictive accuracy.

-   The variable importance plot highlights which predictors contribute most to the model's predictive power. These variables are drought code, temperature, and month. Ecologically, these variables may have a bigger influence on fire and burn area compared to the other variables included in this analysis. Identifying and prioritizing key variables can reduce the number of features in the model without sacrificing accuracy, leading to a more efficient model.

-   Rain and relative humidity have the lowest variable importance scores, indicating that these factors contribute less to the model's predictive power. While these variables may play a role in influencing burn area, their impact is less significant compared to other predictors in the model. These variables can be removed in future model iterations to reduce complexity and minimize overfitting.
